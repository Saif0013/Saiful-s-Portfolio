{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed4RRWJIuuqT"
   },
   "source": [
    "# Week 2 - Demonstration Code\n",
    "\n",
    "Topics covered:\n",
    "* Datasets\n",
    "* Basic Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMmGwFhGw3zb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJkT6ngk_aJu"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed):\n",
    "    \"\"\"Seeds all RNGs with a specific seed value.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EqFj2-eu3Nm"
   },
   "source": [
    "## Learning to Predict a Linear Function\n",
    "\n",
    "Is it possible for a neural network to learn how to predict values from a linear function, just from the data itself?\n",
    "\n",
    "Let's try to see if a neural network can learn how to predict the following function:\n",
    "\n",
    "$ y = 2x $\n",
    "\n",
    "To do this, we will need to:\n",
    "* Create a dataset that creates inputs (x values) and generates ground-truth targets (y values) following the above equation\n",
    "* Define a neural network that takes a single value, and produces a single value\n",
    "* Train the neural network with examples from our dataset and observe if it is able to learn from the data\n",
    "\n",
    "In other words, what we're trying to do is get the neural network to learn what function we used to generate the data, just by only seeing the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwoS-UqBnN06"
   },
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toDTGRb1ysd-"
   },
   "source": [
    "Here we define a basic dataset that creates a set of input values with corresponding target values (that are 2x the input values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmZulC4yCI2q"
   },
   "outputs": [],
   "source": [
    "class BasicLinearFunctionDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "        self.input = np.array(range(0, n))\n",
    "        self.target = self.input * 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        target = self.target[idx]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a205Tg-TLvzj"
   },
   "outputs": [],
   "source": [
    "# Let's create an instance of this dataset and observe pairs of input/targets\n",
    "linear_function_dataset = BasicLinearFunctionDataset(1000)\n",
    "print(linear_function_dataset[5])\n",
    "print(linear_function_dataset[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTzdE7Aky1QR"
   },
   "source": [
    "<font color='red'>Can we write this dataset differently to avoid storing all data ahead of time?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4sfJIIVneva"
   },
   "source": [
    "We can! One way to do this is to compute the next value as we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7xnYbTBMpkC"
   },
   "outputs": [],
   "source": [
    "class BasicLinearFunctionDataset(Dataset):\n",
    "    def __init__(self, n):\n",
    "    ** Fill here **\n",
    "\n",
    "    def __len__(self):\n",
    "     ** Fill here **\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     ** Fill here **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qG4jm78pNStC"
   },
   "outputs": [],
   "source": [
    "linear_function_dataset = BasicLinearFunctionDataset(1000)\n",
    "print(linear_function_dataset[5])\n",
    "print(linear_function_dataset[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCkLlvdjzRxW"
   },
   "source": [
    "Finally, we need to create a dataloader that uses this dataset so we can automatically batch examples (when training), and gives us the opportunity to enable multiprocessing with our dataloading.\n",
    "\n",
    "<font color='red'>Here it would be good to draw some diagram showing the relationship between dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt4mxhmYNy7x"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(linear_function_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Let's look at one of the batches\n",
    "input, target = next(iter(train_dataloader))\n",
    "print('Inputs:', input)\n",
    "print('Targets:',target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7sRD_Ukz9qZ"
   },
   "source": [
    "### The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euzOaylv0DOH"
   },
   "source": [
    "Let's define a basic neural network to try to solve our task of predicting the output value, given an input value.\n",
    "\n",
    "This network should:\n",
    "* Have 3x linear layers (hidden dimensions can be of any size)\n",
    "* Accept a single value as input (This value can be positive or negative)\n",
    "* Output a single value (This value should be allowed to be positive or negative)\n",
    "* Use ReLU non-linear activations where appropriate\n",
    "\n",
    "\n",
    "<font color=\"red\">Below is a definition of a model that aims to follow this criteria, but it has some problems! Can you identify all of the mistakes with this model?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkPt-5L-nqTZ"
   },
   "outputs": [],
   "source": [
    "# Here is the bad code with mistakes\n",
    "# Can you find all the mistakes?\n",
    "class SimpleModel3Layers(nn.Module):\n",
    "    def __init__(self, device):\n",
    "      super().__init__()\n",
    "      self.linear1 = nn.Linear(1, 2)\n",
    "      self.linear2 = nn.Linear(5, 2)\n",
    "      self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.linear1(x)\n",
    "      x = self.linear2(x)\n",
    "      x = nn.ReLU()(nn.Linear(2, 9)(x))\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU8fwGls03mA"
   },
   "source": [
    "Below is a mistake-free version of that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6-y1YTcPD6k"
   },
   "outputs": [],
   "source": [
    "# Here is code that does not have mistakes\n",
    "class SimpleModel3Layers(nn.Module):\n",
    "    def __init__(self, device):\n",
    "      super().__init__()\n",
    "      ** Fill here **\n",
    "      self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "      ** Fill here **\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYo4rF4T1VFt"
   },
   "source": [
    "Next, we will choose the correct device to train on, and test that the forward pass through our network works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsCa2HHmUFkQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Check if GPU acceleration is available (requires a CUDA-compatible GPU) and\n",
    "# set the device variable accordingly. If the computer has more than one GPU,\n",
    "# you can specify which one by replacing 0 with a different index\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "print(\"Training on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-Mwiq6yoPxy"
   },
   "source": [
    "Code to test the forward pass through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p13XNAKdT21I"
   },
   "outputs": [],
   "source": [
    "input, target = next(iter(train_dataloader))\n",
    "print('Input:', input)\n",
    "\n",
    "# We need to put the input onto the same device as the model (i.e. cpu or gpu)\n",
    "input = input.to(device)\n",
    "\n",
    "# We unsqueeze to get dimensionality: [batch_size, 1]\n",
    "input = input.unsqueeze(1)\n",
    "\n",
    "# Here we create an instance of our model and perform a forward pass\n",
    "model = SimpleModel3Layers(device)\n",
    "output = model(input.type(torch.float32))\n",
    "\n",
    "print('Output:', output.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbtsNx0E2meC"
   },
   "source": [
    "Once we have created our model, there are also useful ways we can inspect the model to double check what layers are included with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bvr-IrD2yLw"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFFd-87v25_4"
   },
   "source": [
    "We can also inspect the weights and bias associated to any layer in our network!\n",
    "\n",
    "Let's inspect the linear3 layer.\n",
    "\n",
    "<font color=\"red\">Can you draw a diagram of how the weights and bias are used in combination with the previous layer outputs to produce the next set of outputs?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcXhsTK525Xn"
   },
   "outputs": [],
   "source": [
    "print(model.linear3.weight)\n",
    "print(model.linear3.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFbBqF643ltR"
   },
   "source": [
    "What about for the linear2 layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5X9CISFc3HnG"
   },
   "outputs": [],
   "source": [
    "print(model.linear2.weight)\n",
    "print(model.linear2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ssw4nqiYW-3O"
   },
   "source": [
    "What about for the linear1 layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxsSAmMhwSbT"
   },
   "outputs": [],
   "source": [
    "print(model.linear1.weight)\n",
    "print(model.linear1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTsGuyWOoVcS"
   },
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF8Fhgyv2ZxN"
   },
   "source": [
    "Now we have all the bits and pieces needed to train our model. Let's now try training our model to see if it can learn our function!\n",
    "\n",
    "After running the code cell below, try to modify different parameters to see how that changes your trained model.\n",
    "\n",
    "<font color=\"red\">Please draw a diagram of how the training loop works. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnEM4O1QTJUB"
   },
   "outputs": [],
   "source": [
    "# set the random seed to ensure we get reproducible results\n",
    "seed_all(1234)\n",
    "\n",
    "# Define the dataloader that iterates over our dataset\n",
    "train_dataloader = DataLoader(linear_function_dataset, batch_size=16, shuffle=True)\n",
    "#train_dataloader = DataLoader(linear_function_dataset_normalised, batch_size=16, shuffle=True)\n",
    "#train_dataloader = DataLoader(quadratic_function_dataset_normalised, batch_size=16, shuffle=True)\n",
    "#train_dataloader = DataLoader(advanced_linear_function_dataset_normalised, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the criterion function (to compute the loss). What does MSELoss do?\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the model we want to train\n",
    "model = SimpleModel3Layers(device)\n",
    "#model = SimpleModel1Layer(device)\n",
    "# model = SimpleModel1LayerNoBias(device)\n",
    "\n",
    "# Define the optimizer we want to use. What does the optimizer do? What about the initial learning rate?\n",
    "# Does changing the learning rate impact training?\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of epochs we want to train for. An epoch is a complete iteration over our dataset\n",
    "num_epochs = 400\n",
    "\n",
    "# Put the model into train mode. What does this do?\n",
    "model.train()\n",
    "\n",
    "train_loss = []\n",
    "for epoch in tqdm(range(num_epochs)):  # loop over the dataset multiple times\n",
    "    loss = 0.0\n",
    "    for data in train_dataloader:       # Get the next batch from the dataloader\n",
    "        # 1. Extract the inputs/targets from the batch\n",
    "        inputs, targets = data\n",
    "\n",
    "        # 2. Copy the data to the specified device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # 3. Turn our inputs and targets from dimensionality: [batch_size] to dimensionality: [batch_size, 1] (required by model)\n",
    "        #    This is not always required, but for this task it is\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        targets = targets.unsqueeze(1)\n",
    "\n",
    "        # 4. Set all parameter gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 5. Have the model make a prediction for the batch\n",
    "        outputs = model(inputs.type(torch.float32))\n",
    "\n",
    "        # 6. Compute the loss\n",
    "        loss = criterion(outputs, targets.type(torch.float32))\n",
    "\n",
    "        # 7. Backpropagate the loss to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 8. Update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += loss.item()\n",
    "\n",
    "    train_loss.append(loss.detach().cpu().numpy() / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KER-VsIv4-8I"
   },
   "source": [
    "Let's plot the training loss per-epoch so we can inspect how the model performed. Does this tell us anything about if the model learned from our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wokceGPaM6yI"
   },
   "outputs": [],
   "source": [
    "epochs = range(len(train_loss))\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epochs, train_loss, 'r-', label='Training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPa6UoG_j0-C"
   },
   "source": [
    "Let's now closely inspect exactly how well our trained model actually performs.\n",
    "\n",
    "Here we sample a batch from our dataloader, make predictions on it with our trained model, and display both the predictions and our targets. Are they similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDlE_h3SarMe"
   },
   "outputs": [],
   "source": [
    "input, target = next(iter(train_dataloader))\n",
    "input = input.to(device)\n",
    "print('Input:', input)\n",
    "print('\\nTarget:', target)\n",
    "input = input.unsqueeze(1)\n",
    "output = model(input.type(torch.float32))\n",
    "\n",
    "print('\\nOutput:', output.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_-bRifA5tbe"
   },
   "source": [
    "### A Modified Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pIt4asl5xv4"
   },
   "source": [
    "When looking at the loss curve, you might notice that the value of the loss is extremely high!\n",
    "\n",
    "One reason for this is that the values in our dataset vary by so much (i.e. inputs from 0 to 1000, targets from 0 to 2000), that the scale of the loss can quickly grow. In some cases this can be a problem and cause our model to take a much longer time to converge.\n",
    "\n",
    "In the code cell below, we redefine the dataset such to ensure that the distribution of our input values have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "This is achieved by translating all of our input values by:\n",
    "\n",
    "$ input = \\frac{input - \\mu}{\\sigma} $\n",
    "\n",
    "Where:\n",
    "* $\\mu$ is the mean of all input values in our dataset\n",
    "* $\\sigma$ is the standard deviation of all input values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6PqLdk2dN1U"
   },
   "outputs": [],
   "source": [
    "class BasicLinearFunctionDatasetNormalised(Dataset):\n",
    "    def __init__(self, n):\n",
    "        self.input = np.array(range(0, n))\n",
    "        self.input_mean = self.input.mean()\n",
    "        self.input_std = self.input.std()\n",
    "        self.input = (self.input - self.input_mean) / self.input_std\n",
    "        self.target = self.input * 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        target = self.target[idx]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-q4JHMYec30"
   },
   "outputs": [],
   "source": [
    "linear_function_dataset_normalised = BasicLinearFunctionDatasetNormalised(1000)\n",
    "print(linear_function_dataset_normalised[5])\n",
    "print(linear_function_dataset_normalised[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFDn7SBo7r6I"
   },
   "source": [
    "Let's now recreate the dataloader and try this task again.\n",
    "\n",
    "<font color=\"red\">Go back to the training loop and modify the dataloader to use the *linear_function_dataset_normalised* dataset object. Retrain your model, and inspect the loss curve and model outputs to see if the model is performing better now</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUPAnbrx1PvK"
   },
   "source": [
    "### A Simpler Model\n",
    "Can we create an even simpler model to solve this task?\n",
    "\n",
    "Given our task is to learn the linear function $ y = 2x $, can we achieve this with a single linear layer (and no non-linear activation functions)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUlkr2cT1P17"
   },
   "outputs": [],
   "source": [
    "class SimpleModel1Layer(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "      super().__init__()\n",
    "      ** Fill here **\n",
    "      self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "      ** Fill here **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWUpbytW_AA8"
   },
   "source": [
    "<font color=\"red\">Go back to the training loop and modify the code to use this model. Retrain your model, and inspect the loss curve and model outputs. Was this model sufficient?</font>\n",
    "\n",
    "Once you've done that, come back here and inspect the weight and bias of the linear layer in the model. What do you think they should be if the model is well converged on this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1LBs7BmAdrL"
   },
   "outputs": [],
   "source": [
    "print(model.linear1.weight)\n",
    "print(model.linear1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slXC6ni0_QUY"
   },
   "source": [
    "## Other Tasks\n",
    "\n",
    "Here are a set of other tasks that you should try to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyifFLeh_XSv"
   },
   "source": [
    "### Learning to Predict a Quadratic Function\n",
    "\n",
    "Is it possible to get a model to learn a slightly more complex function, such as:\n",
    "\n",
    "$ y = x^{2} $\n",
    "\n",
    "Below is a dataset that will return targets in this form.\n",
    "\n",
    "Before training your model, try to answer these questions:\n",
    "* Do you think the SimpleModel3Layers model will be able to learn this task?\n",
    "* If we remove the non-linear activations from the SimpleModel3Layers model, will it be able to learn this task?\n",
    "* If we use the SimpleModel1Layer model, will it be able to learn this task?\n",
    "\n",
    "Use this dataset in the training loop above and see if you can answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ3L38hZ_WvH"
   },
   "outputs": [],
   "source": [
    "class BasicQuadraticFunctionDatasetNormalised(Dataset):\n",
    "    def __init__(self, n):\n",
    "        self.input = np.array(range(0, n))\n",
    "        self.input_mean = self.input.mean()\n",
    "        self.input_std = self.input.std()\n",
    "        self.input = (self.input - self.input_mean) / self.input_std\n",
    "        self.target = self.input ** 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        target = self.target[idx]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trVaC8O7_VX6"
   },
   "outputs": [],
   "source": [
    "# Create an instance of this dataset and observe pairs of input/targets\n",
    "quadratic_function_dataset_normalised = BasicQuadraticFunctionDatasetNormalised(1000)\n",
    "print(quadratic_function_dataset_normalised[5])\n",
    "print(quadratic_function_dataset_normalised[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5ppIJakA-7b"
   },
   "outputs": [],
   "source": [
    "class SimpleModel3LayersNoActivations(nn.Module):\n",
    "    def __init__(self, device):\n",
    "      super().__init__()\n",
    "      self.linear1 = nn.Linear(1, 5)\n",
    "      self.linear2 = nn.Linear(5, 2)\n",
    "      self.linear3 = nn.Linear(2, 1)\n",
    "      self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.linear1(x)\n",
    "      x = self.linear2(x)\n",
    "      x = self.linear3(x)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAnyZt5JAzvp"
   },
   "source": [
    "### Learning to Predict a More Advanced Linear Function\n",
    "\n",
    "What if we modify the linear function to also have a constant, such as:\n",
    "\n",
    "$ y = 2x + 100 $\n",
    "\n",
    "Below is a dataset that will return targets in this form.\n",
    "\n",
    "Before training your model, try to answer these questions:\n",
    "* Do you think the SimpleModel1Layer model, will it be able to learn this task? If so, what do you think the weight and bias of this model will be if training is successful?\n",
    "* Do you think an adapted version of the SimpleModel1Layer model (with no bias in the linear layer) will be able to learn this task?\n",
    "\n",
    "Use this dataset and model in the training loop above and see if you can answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEgyL-v9BXa1"
   },
   "outputs": [],
   "source": [
    "class AdvancedLinearFunctionDatasetNormalised(Dataset):\n",
    "    def __init__(self, n):\n",
    "        self.input = np.array(range(0, n))\n",
    "        self.input_mean = self.input.mean()\n",
    "        self.input_std = self.input.std()\n",
    "        self.input = (self.input - self.input_mean) / self.input_std\n",
    "        self.target = self.input * 2 + 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        target = self.target[idx]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUgrJfLCBgHN"
   },
   "outputs": [],
   "source": [
    "advanced_linear_function_dataset_normalised = AdvancedLinearFunctionDatasetNormalised(1000)\n",
    "print(advanced_linear_function_dataset_normalised[5])\n",
    "print(advanced_linear_function_dataset_normalised[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRwpgcixBk1J"
   },
   "outputs": [],
   "source": [
    "class SimpleModel1LayerNoBias(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "      super().__init__()\n",
    "      self.linear1 = nn.Linear(1, 1, bias=False)\n",
    "      self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.linear1(x)\n",
    "      return x"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1Fzqs3S-4CQjphw8Rw3yoOOOFpPJwJDZT",
     "timestamp": 1678673259660
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
