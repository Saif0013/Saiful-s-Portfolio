{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNnPxmhPwRBz"
   },
   "source": [
    "# Workshop 1 \n",
    "In this workshop, you are going to learn about utlizing regular expressions for text matching, NLTK, Spacy for NLP as well as a visualization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIaTvkJrePOZ"
   },
   "source": [
    "## 1- Regular expressions\n",
    "\n",
    "Regular expressions are a powerful language for matching text patterns. This page gives a basic introduction to regular expressions themselves sufficient for our Python exercises and shows how regular expressions work in Python. The Python \"re\" module provides regular expression support.\n",
    "In Python a regular expression search is typically written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0ufRRVBbM_5z"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pat = ''\n",
    "test_str = '' \n",
    "match = re.search(pat, test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMCovEdmNMrs"
   },
   "source": [
    "The re.search() method takes a regular expression pattern and a string and searches for that pattern within the string. If the search is successful, search() returns a match object or None otherwise. As shown in the following example which searches for the pattern 'word:' followed by a 3 letter word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NUvXZlr4NYJE",
    "outputId": "f32d0737-f390-41e3-90bd-792554c767a7"
   },
   "outputs": [],
   "source": [
    "test_str = 'an example word:cat!!'\n",
    "match = re.search(r'word:\\w+', test_str)\n",
    "match.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsO-3B3-N3hg"
   },
   "source": [
    "The code ```match = re.search(pat, str)``` stores the search result in a variable named \"match\".\n",
    "\n",
    "The 'r' at the start of the pattern string designates a python \"raw\" string which passes through backslashes without change which is very handy for regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh3mUKwYOZ3F"
   },
   "source": [
    "### Basic Patterns\n",
    "\n",
    "The power of regular expressions is that they can specify patterns, not just fixed characters. Here are the most basic patterns which match single chars:\n",
    "\n",
    "* a, X, 9, < -- ordinary characters just match themselves exactly. The meta-characters which do not match themselves because they have special meanings are: . ^ $ * + ? { [ ] \\ | ( ) (details below)\n",
    "\n",
    "* . (a period) -- matches any single character except newline '\\n'\n",
    "* \\w -- (lowercase w) matches a \"word\" character: a letter or digit or underbar [a-zA-Z0-9_]. Note that although \"word\" is the mnemonic for this, it only matches a single word char, not a whole word. \\W (upper case W) matches any non-word character.\n",
    "* \\b -- boundary between word and non-word\n",
    "* \\s -- (lowercase s) matches a single whitespace character -- space, newline, return, tab, form [ \\n\\r\\t\\f]. \\S (upper case S) matches any non-whitespace character.\n",
    "* \\t, \\n, \\r -- tab, newline, return\n",
    "* \\d -- decimal digit [0-9] (some older regex utilities do not support but \\d, but they all support \\w and \\s)\n",
    "* ^ = start, $ = end -- match the start or end of the string\n",
    "* \\ -- inhibit the \"specialness\" of a character. So, for example, use \\. to match a period or \\\\ to match a slash. If you are unsure if a character has special meaning, such as '@', you can put a slash in front of it, \\@, to make sure it is treated just as a character.\n",
    "\n",
    "For more details please refer to the week 2 slides and the chapter provided for further reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2YrJ_P9Q_2J"
   },
   "source": [
    "### Basic Examples\n",
    "\n",
    "The basic rules of regular expression search for a pattern within a string are:\n",
    "\n",
    "The search proceeds through the string from start to end, stopping at the first match found\n",
    "All of the pattern must be matched, but not all of the string\n",
    "If ```match = re.search(pat, str)``` is successful, match is not None and in particular match.group() is the matching text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErCDGsN6RPoZ",
    "outputId": "23a53585-fcfc-4ec2-e7fd-b520c22a4d23"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'iii', 'piiig') \n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNH9RJrBRrq4",
    "outputId": "48a05bc1-c7f6-40ed-d0ca-a82bf35aed57"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'igs', 'piiig') \n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihRRPm9WRz_u",
    "outputId": "55c4f4d5-e81e-42a5-fed8-95fb3f73380e"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'..g', 'piiig')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAcXil_CR5tW",
    "outputId": "c3b67f6e-9f23-467c-bbb3-0b14b5fbe791"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'\\d\\d\\d', 'p123g') \n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRLXkA6YR_eX",
    "outputId": "1150a7cb-8b2a-46e1-fb7c-e30da9bc76e6"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'\\w\\w\\w', '@@abcd!!') \n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fo4KyNiZSh0z"
   },
   "source": [
    "### Repetition\n",
    "\n",
    "Things get more interesting when you use + and * to specify repetition in the pattern\n",
    "\n",
    "* \\+ -- 1 or more occurrences of the pattern to its left, e.g. 'i+' = one or more i's\n",
    "* \\* -- 0 or more occurrences of the pattern to its left\n",
    "* ? -- match 0 or 1 occurrences of the pattern to its left\n",
    "\n",
    "### Leftmost & Largest\n",
    "First the search finds the leftmost match for the pattern, and second it tries to use up as much of the string as possible -- i.e. + and * go as far as possible (the + and * are said to be \"greedy\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHfLkDU2TQav"
   },
   "source": [
    "### Repetition Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2_310eHTOup",
    "outputId": "91073339-de3d-48c6-e566-182ab6db65c4"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'pi+', 'piiig')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m5okuikWTZVt",
    "outputId": "984f4e75-16f8-4f73-ff87-ab309f76610a"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'i+', 'piigiiii')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awT6c4ymUIx5",
    "outputId": "c9c914e6-0f82-48ef-da3e-7cff928f2d1c"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K04_TBAAUNKJ",
    "outputId": "0cc91728-1453-434b-837e-10dfcfa4e9d4"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DseUiZIDUQpt",
    "outputId": "07018206-c46c-414b-b4f7-d108ff998bc2"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZSWCSqpUizv",
    "outputId": "e03aaee3-4721-4344-d367-e64d9447c2b2"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'^b\\w+', 'foobar')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwKlmzgVUr9b",
    "outputId": "4b064ccf-2931-44ba-dcd0-d7194e13e189"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'b\\w+', 'foobar')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XV0ZgDvUxSH"
   },
   "source": [
    "### Emails Example\n",
    "\n",
    "Suppose you want to find the email address inside the string 'xyz alice-b@students.latrobe.edu.au purple monkey'. We'll use this as a running example to demonstrate more regular expression features. Here's an attempt using the pattern r'\\w+@\\w+':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9JCedzmVCYp",
    "outputId": "1685c04e-0607-4b97-a465-55801ac6252d"
   },
   "outputs": [],
   "source": [
    "test_email = '1112-2233@students.latrobe.edu.au monkey dishwasher'\n",
    "match = re.search(r'\\w+@\\w+', test_email)\n",
    "print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvRLqoK-ZGvr"
   },
   "source": [
    "The search does not get the whole email address in this case because the \\w does not match the '-' or '.' in the address. We'll fix this using the regular expression features below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbdUc_S0ZVYS",
    "outputId": "f864ea00-a3a1-43d5-b04b-8c72f13df543"
   },
   "outputs": [],
   "source": [
    "match = re.search(r'[\\w.-]+@[\\w.-]+', test_email)\n",
    "print(match.group() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBSnxX3iaLf4"
   },
   "source": [
    "### Group Extraction\n",
    "\n",
    "The \"group\" feature of a regular expression allows you to pick out parts of the matching text. Suppose for the emails problem that we want to extract the username and host separately. To do this, add parenthesis ( ) around the username and host in the pattern, like this: r'([\\w.-]+)@([\\w.-]+)'. In this case, the parenthesis do not change what the pattern will match, instead they establish logical \"groups\" inside of the match text. On a successful search, match.group(1) is the match text corresponding to the 1st left parenthesis, and match.group(2) is the text corresponding to the 2nd left parenthesis. The plain match.group() is still the whole match text as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4m-UlqTaSaZ",
    "outputId": "9034140f-d79a-4936-eaac-f7db4583fdd5"
   },
   "outputs": [],
   "source": [
    "test_str = 'purple 1112-2233@students.latrobe.edu.au monkey dishwasher'\n",
    "match = re.search(r'([\\w.-]+)@([\\w.-]+)', test_str)\n",
    "print(match.group())\n",
    "print(match.groups())  \n",
    "print(match.group(1)) \n",
    "print(match.group(2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9F8xg2kua2rg"
   },
   "source": [
    "### findall\n",
    "\n",
    "findall() is probably the single most powerful function in the re module. Above we used re.search() to find the first match for a pattern. findall() finds *all* the matches and returns them as a list of strings, with each string representing one match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5-jPtnOa-kX",
    "outputId": "02584101-d58f-49f9-f711-3fd80d1894c8"
   },
   "outputs": [],
   "source": [
    "test_str = 'purple 1112-2233@students.latrobe.edu.au, blah monkey bob@abc.com blah dishwasher'\n",
    "\n",
    "emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', test_str) \n",
    "for email in emails:\n",
    "  # do something with each found email string\n",
    "  print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHUvi546qvDW"
   },
   "source": [
    "### Execise 1.1\n",
    "\n",
    "Write regular expressions to capture the following requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bnGSit5q8jl"
   },
   "source": [
    "#### 1. cleanup_address\n",
    "Given \n",
    "* 50 Fifth Ave. New York, NY 10012\n",
    "* 100 Ninth Ave. Brooklyn, NY 11416\n",
    "* 9 Houston St. Juneau, AK 99999\n",
    "* 2800 Springfield Rd. Omaha, NE 55555\n",
    "\n",
    "Change to:\n",
    "* 50,Fifth Ave.,New York,NY,10012\n",
    "* 100,Ninth Ave.,Brooklyn,NY,11416\n",
    "* 9,Houston St.,Juneau,AK,99999\n",
    "* 2800,Springfield Rd.,Omaha,NE,55555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBPvdupuqqhf",
    "outputId": "af9e154a-4323-4d22-e691-921b8a287397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50,Fifth Ave.,New York,NY,10012 100,Ninth Ave.,Brooklyn,NY,11416 9,Houston St.,Juneau,AK,99999 2800,Springfield Rd.,Omaha,NE,55555\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "def cleanup_address(address):\n",
    "  match = re.search(r'(\\d+)\\s(\\w+\\s\\w+\\.)\\s([\\w\\s]+)\\,\\s([\\w]{2})\\s(\\d{5})', address)\n",
    "  return match.group(1)+','+match.group(2)+','+match.group(3)+','+match.group(4)+','+match.group(5)\n",
    "\n",
    "addresses = ['50 Fifth Ave. New York, NY 10012',\n",
    "             '100 Ninth Ave. Brooklyn, NY 11416',\n",
    "             '9 Houston St. Juneau, AK 99999',\n",
    "             '2800 Springfield Rd. Omaha, NE 55555']\n",
    "\n",
    "print(*(cleanup_address(address) for address in addresses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBuW6FNWcCre"
   },
   "source": [
    "#### 2. extract_names\n",
    "\n",
    "The Social Security administration has this neat data by year of what names are most popular for babies born that year in the USA (see [social security baby names](http://www.socialsecurity.gov/OACT/babynames/)).\n",
    "\n",
    "Implement the extract_names(filename) function which takes the filename of a baby1990.html file and returns the data from the file as a single list -- the year string at the start of the list followed by the name-rank strings in alphabetical order. ```['2006', 'Aaliyah 91', 'Abagail 895', 'Aaron 57', ...]```. Note that for parsing webpages in general, regular expressions don't do a good job, but these webpages have a simple and consistent format.\n",
    "\n",
    "Rather than treat the boy and girl names separately, we'll just lump them all together. In some years, a name appears more than once in the html, but we'll just use one number per name. Optional: make the algorithm smart about this case and choose whichever number is smaller.\n",
    "\n",
    "Build the program as a series of small milestones, getting each step to run/print something before trying the next step. This is the pattern used by experienced programmers -- build a series of incremental milestones, each with some output to check, rather than building the whole program in one huge step.\n",
    "\n",
    "Printing the data you have at the end of one milestone helps you think about how to re-structure that data for the next milestone. Here are some suggested milestones:\n",
    "\n",
    "* Extract all the text from the file and print it\n",
    "* Find and extract the year and print it\n",
    "* Extract the names and rank numbers and print them\n",
    "* Get the names data into a dict and print it\n",
    "* Build the [year, 'name rank', ... ] list and print it\n",
    "\n",
    "The function should return the list.\n",
    "\n",
    "Tto make the list into a reasonable looking summary text for printing, here's a clever use of join: text = '\\n'.join(mylist) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TtOtdRmlgWp6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2009',\n",
       " 'Aaden 343',\n",
       " 'Aaliyah 77',\n",
       " 'Aarav 921',\n",
       " 'Aaron 50',\n",
       " 'Abagail 874',\n",
       " 'Abbey 822',\n",
       " 'Abbie 737',\n",
       " 'Abbigail 508',\n",
       " 'Abby 259',\n",
       " 'Abdiel 984',\n",
       " 'Abdullah 953',\n",
       " 'Abel 360',\n",
       " 'Abigail 8',\n",
       " 'Abigayle 936',\n",
       " 'Abraham 187',\n",
       " 'Abram 548',\n",
       " 'Abril 522',\n",
       " 'Ace 714',\n",
       " 'Ada 598',\n",
       " 'Adalyn 526',\n",
       " 'Adalynn 845',\n",
       " 'Adam 72',\n",
       " 'Adan 299',\n",
       " 'Addison 12',\n",
       " 'Addison 820',\n",
       " 'Addisyn 556',\n",
       " 'Addyson 240',\n",
       " 'Adelaide 551',\n",
       " 'Adeline 361',\n",
       " 'Adelyn 577',\n",
       " 'Aden 254',\n",
       " 'Adison 900',\n",
       " 'Aditya 702',\n",
       " 'Adolfo 731',\n",
       " 'Adonis 854',\n",
       " 'Adrian 56',\n",
       " 'Adriana 124',\n",
       " 'Adrianna 173',\n",
       " 'Adriel 584',\n",
       " 'Adrien 727',\n",
       " 'Adrienne 786',\n",
       " 'Adyson 648',\n",
       " 'Aedan 821',\n",
       " 'Agustin 740',\n",
       " 'Ahmad 559',\n",
       " 'Ahmed 576',\n",
       " 'Aidan 59',\n",
       " 'Aiden 16',\n",
       " 'Aidyn 699',\n",
       " 'Aileen 483',\n",
       " 'Aimee 779',\n",
       " 'Ainsley 441',\n",
       " 'Aisha 764',\n",
       " 'Aiyana 595',\n",
       " 'Akira 705',\n",
       " 'Alaina 223',\n",
       " 'Alan 135',\n",
       " 'Alana 166',\n",
       " 'Alani 913',\n",
       " 'Alanna 461',\n",
       " 'Alannah 938',\n",
       " 'Alayna 285',\n",
       " 'Albert 372',\n",
       " 'Alberto 338',\n",
       " 'Alden 916',\n",
       " 'Aldo 569',\n",
       " 'Aleah 627',\n",
       " 'Alec 390',\n",
       " 'Aleena 664',\n",
       " 'Alejandra 255',\n",
       " 'Alejandro 105',\n",
       " 'Alena 686',\n",
       " 'Alessandra 407',\n",
       " 'Alessandro 624',\n",
       " 'Alex 85',\n",
       " 'Alexa 50',\n",
       " 'Alexander 6',\n",
       " 'Alexandra 61',\n",
       " 'Alexandria 189',\n",
       " 'Alexia 213',\n",
       " 'Alexis 15',\n",
       " 'Alexis 169',\n",
       " 'Alexus 721',\n",
       " 'Alexzander 674',\n",
       " 'Alfonso 585',\n",
       " 'Alfred 787',\n",
       " 'Alfredo 388',\n",
       " 'Ali 398',\n",
       " 'Ali 866',\n",
       " 'Alia 929',\n",
       " 'Aliana 907',\n",
       " 'Alice 326',\n",
       " 'Alicia 178',\n",
       " 'Alijah 469',\n",
       " 'Alina 245',\n",
       " 'Alisa 867',\n",
       " 'Alisha 642',\n",
       " 'Alison 273',\n",
       " 'Alissa 438',\n",
       " 'Alisson 783',\n",
       " 'Alivia 181',\n",
       " 'Aliya 668',\n",
       " 'Aliyah 192',\n",
       " 'Aliza 861',\n",
       " 'Allan 552',\n",
       " 'Allen 294',\n",
       " 'Allie 230',\n",
       " 'Allison 32',\n",
       " 'Allisson 418',\n",
       " 'Ally 766',\n",
       " 'Allyson 266',\n",
       " 'Alma 792',\n",
       " 'Alondra 201',\n",
       " 'Alonso 660',\n",
       " 'Alonzo 542',\n",
       " 'Alvaro 639',\n",
       " 'Alvin 437',\n",
       " 'Alyson 480',\n",
       " 'Alyssa 16',\n",
       " 'Alyvia 970',\n",
       " 'Amanda 138',\n",
       " 'Amani 583',\n",
       " 'Amara 578',\n",
       " 'Amare 677',\n",
       " 'Amari 344',\n",
       " 'Amari 393',\n",
       " 'Amaris 987',\n",
       " 'Amaya 198',\n",
       " 'Amber 165',\n",
       " 'Amelia 68',\n",
       " 'Amelie 781',\n",
       " 'America 503',\n",
       " 'Amiah 963',\n",
       " 'Amina 763',\n",
       " 'Amir 297',\n",
       " 'Amira 436',\n",
       " 'Amirah 765',\n",
       " 'Amiya 660',\n",
       " 'Amiyah 479',\n",
       " 'Amy 132',\n",
       " 'Amya 493',\n",
       " 'Ana 169',\n",
       " 'Anabel 955',\n",
       " 'Anabella 919',\n",
       " 'Anabelle 710',\n",
       " 'Anahi 437',\n",
       " 'Anastasia 360',\n",
       " 'Anaya 439',\n",
       " 'Anderson 326',\n",
       " 'Andre 220',\n",
       " 'Andrea 71',\n",
       " 'Andreas 969',\n",
       " 'Andres 158',\n",
       " 'Andrew 12',\n",
       " 'Andy 211',\n",
       " 'Angel 160',\n",
       " 'Angel 36',\n",
       " 'Angela 133',\n",
       " 'Angelica 257',\n",
       " 'Angelina 69',\n",
       " 'Angeline 784',\n",
       " 'Angelique 687',\n",
       " 'Angelo 284',\n",
       " 'Angie 340',\n",
       " 'Anika 573',\n",
       " 'Aniya 272',\n",
       " 'Aniyah 187',\n",
       " 'Ann 830',\n",
       " 'Anna 26',\n",
       " 'Annabel 801',\n",
       " 'Annabella 589',\n",
       " 'Annabelle 188',\n",
       " 'Annalise 702',\n",
       " 'Anne 499',\n",
       " 'Annie 392',\n",
       " 'Annika 402',\n",
       " 'Ansley 719',\n",
       " 'Anthony 7',\n",
       " 'Antoine 693',\n",
       " 'Anton 832',\n",
       " 'Antonio 102',\n",
       " 'Antony 895',\n",
       " 'Antwan 879',\n",
       " 'Anya 375',\n",
       " 'April 333',\n",
       " 'Arabella 657',\n",
       " 'Araceli 767',\n",
       " 'Aracely 868',\n",
       " 'Areli 848',\n",
       " 'Arely 610',\n",
       " 'Ari 532',\n",
       " 'Aria 575',\n",
       " 'Ariana 81',\n",
       " 'Arianna 66',\n",
       " 'Ariel 217',\n",
       " 'Ariel 570',\n",
       " 'Ariella 923',\n",
       " 'Arielle 619',\n",
       " 'Arjun 755',\n",
       " 'Armando 293',\n",
       " 'Armani 583',\n",
       " 'Armani 755',\n",
       " 'Arnav 974',\n",
       " 'Aron 600',\n",
       " 'Arthur 363',\n",
       " 'Arturo 352',\n",
       " 'Aryan 669',\n",
       " 'Aryana 753',\n",
       " 'Aryanna 613',\n",
       " 'Asa 633',\n",
       " 'Ashanti 930',\n",
       " 'Asher 206',\n",
       " 'Ashlee 535',\n",
       " 'Ashleigh 927',\n",
       " 'Ashley 18',\n",
       " 'Ashly 838',\n",
       " 'Ashlyn 153',\n",
       " 'Ashlynn 282',\n",
       " 'Ashton 121',\n",
       " 'Ashtyn 975',\n",
       " 'Asia 386',\n",
       " 'Aspen 700',\n",
       " 'Athena 455',\n",
       " 'Atticus 689',\n",
       " 'Aubree 276',\n",
       " 'Aubrey 42',\n",
       " 'Aubrie 470',\n",
       " 'Audrey 44',\n",
       " 'Audrina 353',\n",
       " 'August 482',\n",
       " 'Augustus 795',\n",
       " 'Aurora 288',\n",
       " 'Austin 55',\n",
       " 'Autumn 89',\n",
       " 'Ava 5',\n",
       " 'Avah 972',\n",
       " 'Averi 966',\n",
       " 'Averie 768',\n",
       " 'Avery 216',\n",
       " 'Avery 38',\n",
       " 'Axel 272',\n",
       " 'Ayaan 908',\n",
       " 'Ayana 814',\n",
       " 'Ayanna 579',\n",
       " 'Aydan 567',\n",
       " 'Ayden 91',\n",
       " 'Aydin 627',\n",
       " 'Ayla 406',\n",
       " 'Aylin 557',\n",
       " 'Azaria 954',\n",
       " 'Azul 611',\n",
       " 'Bailee 513',\n",
       " 'Bailey 790',\n",
       " 'Bailey 82',\n",
       " 'Barbara 758',\n",
       " 'Baron 991',\n",
       " 'Barrett 609',\n",
       " 'Baylee 419',\n",
       " 'Beatrice 833',\n",
       " 'Beau 397',\n",
       " 'Beckett 480',\n",
       " 'Beckham 893',\n",
       " 'Belen 869',\n",
       " 'Belinda 747',\n",
       " 'Bella 122',\n",
       " 'Ben 611',\n",
       " 'Benjamin 25',\n",
       " 'Bennett 361',\n",
       " 'Bentley 937',\n",
       " 'Bernard 940',\n",
       " 'Bethany 318',\n",
       " 'Bianca 204',\n",
       " 'Billy 564',\n",
       " 'Blaine 614',\n",
       " 'Blake 90',\n",
       " 'Blaze 866',\n",
       " 'Bo 840',\n",
       " 'Bobby 525',\n",
       " 'Boston 563',\n",
       " 'Braden 156',\n",
       " 'Bradley 201',\n",
       " 'Brady 94',\n",
       " 'Bradyn 612',\n",
       " 'Braeden 346',\n",
       " 'Braedon 822',\n",
       " 'Braelyn 563',\n",
       " 'Braiden 494',\n",
       " 'Branden 586',\n",
       " 'Brandon 33',\n",
       " 'Branson 873',\n",
       " 'Braxton 202',\n",
       " 'Brayan 333',\n",
       " 'Brayden 51',\n",
       " 'Braydon 325',\n",
       " 'Braylen 572',\n",
       " 'Braylon 252',\n",
       " 'Breanna 168',\n",
       " 'Brenda 342',\n",
       " 'Brendan 207',\n",
       " 'Brenden 353',\n",
       " 'Brendon 578',\n",
       " 'Brenna 420',\n",
       " 'Brennan 263',\n",
       " 'Brennen 651',\n",
       " 'Brent 558',\n",
       " 'Brenton 975',\n",
       " 'Brett 311',\n",
       " 'Bria 924',\n",
       " 'Brian 87',\n",
       " 'Briana 145',\n",
       " 'Brianna 23',\n",
       " 'Brice 710',\n",
       " 'Bridger 978',\n",
       " 'Bridget 394',\n",
       " 'Brielle 338',\n",
       " 'Briley 819',\n",
       " 'Brisa 787',\n",
       " 'Britney 689',\n",
       " 'Brittany 421',\n",
       " 'Brock 277',\n",
       " 'Broderick 899',\n",
       " 'Brodie 421',\n",
       " 'Brody 70',\n",
       " 'Brogan 833',\n",
       " 'Bronson 987',\n",
       " 'Brooke 52',\n",
       " 'Brooklyn 47',\n",
       " 'Brooklynn 182',\n",
       " 'Brooks 601',\n",
       " 'Bruce 476',\n",
       " 'Bruno 801',\n",
       " 'Bryan 82',\n",
       " 'Bryanna 512',\n",
       " 'Bryant 404',\n",
       " 'Bryce 116',\n",
       " 'Brycen 483',\n",
       " 'Brylee 531',\n",
       " 'Brynlee 895',\n",
       " 'Brynn 323',\n",
       " 'Bryson 178',\n",
       " 'Byron 451',\n",
       " 'Cade 310',\n",
       " 'Caden 95',\n",
       " 'Cadence 212',\n",
       " 'Cael 834',\n",
       " 'Caiden 250',\n",
       " 'Cailyn 738',\n",
       " 'Caitlin 235',\n",
       " 'Caitlyn 227',\n",
       " 'Cale 623',\n",
       " 'Caleb 34',\n",
       " 'Cali 426',\n",
       " 'Callie 233',\n",
       " 'Callum 976',\n",
       " 'Calvin 228',\n",
       " 'Camden 204',\n",
       " 'Cameron 320',\n",
       " 'Cameron 53',\n",
       " 'Camila 83',\n",
       " 'Camilla 634',\n",
       " 'Camille 300',\n",
       " 'Campbell 762',\n",
       " 'Camren 675',\n",
       " 'Camron 445',\n",
       " 'Camryn 244',\n",
       " 'Camryn 900',\n",
       " 'Cannon 631',\n",
       " 'Cara 463',\n",
       " 'Carina 901',\n",
       " 'Carissa 727',\n",
       " 'Carl 490',\n",
       " 'Carla 584',\n",
       " 'Carlee 701',\n",
       " 'Carleigh 996',\n",
       " 'Carley 637',\n",
       " 'Carlie 673',\n",
       " 'Carlo 992',\n",
       " 'Carlos 73',\n",
       " 'Carly 197',\n",
       " 'Carmelo 941',\n",
       " 'Carmen 262',\n",
       " 'Carolina 355',\n",
       " 'Caroline 94',\n",
       " 'Carolyn 678',\n",
       " 'Carsen 993',\n",
       " 'Carson 89',\n",
       " 'Carter 65',\n",
       " 'Case 956',\n",
       " 'Casey 336',\n",
       " 'Casey 466',\n",
       " 'Cash 253',\n",
       " 'Cason 571',\n",
       " 'Cassandra 284',\n",
       " 'Cassidy 228',\n",
       " 'Cassie 908',\n",
       " 'Cassius 945',\n",
       " 'Catalina 707',\n",
       " 'Catherine 149',\n",
       " 'Cayden 173',\n",
       " 'Caylee 519',\n",
       " 'Cecelia 756',\n",
       " 'Cecilia 270',\n",
       " 'Cedric 650',\n",
       " 'Celeste 397',\n",
       " 'Celia 757',\n",
       " 'Cesar 165',\n",
       " 'Chace 655',\n",
       " 'Chad 454',\n",
       " 'Chaim 858',\n",
       " 'Chana 971',\n",
       " 'Chance 255',\n",
       " 'Chandler 457',\n",
       " 'Chanel 775',\n",
       " 'Charity 793',\n",
       " 'Charlee 820',\n",
       " 'Charles 63',\n",
       " 'Charlie 307',\n",
       " 'Charlie 736',\n",
       " 'Charlize 905',\n",
       " 'Charlotte 87',\n",
       " 'Chase 67',\n",
       " 'Chasity 956',\n",
       " 'Chaya 749',\n",
       " 'Chaz 959',\n",
       " 'Chelsea 221',\n",
       " 'Cherish 739',\n",
       " 'Cheyanne 496',\n",
       " 'Cheyenne 159',\n",
       " 'Chloe 10',\n",
       " 'Chris 348',\n",
       " 'Christian 23',\n",
       " 'Christina 200',\n",
       " 'Christine 539',\n",
       " 'Christopher 9',\n",
       " 'Ciara 324',\n",
       " 'Cierra 788',\n",
       " 'Cindy 434',\n",
       " 'Claire 62',\n",
       " 'Clara 206',\n",
       " 'Clare 679',\n",
       " 'Clarence 938',\n",
       " 'Clarissa 560',\n",
       " 'Clark 698',\n",
       " 'Claudia 417',\n",
       " 'Clay 849',\n",
       " 'Clayton 233',\n",
       " 'Clinton 856',\n",
       " 'Cloe 915',\n",
       " 'Coby 835',\n",
       " 'Cody 122',\n",
       " 'Cohen 356',\n",
       " 'Colby 273',\n",
       " 'Cole 84',\n",
       " 'Coleman 741',\n",
       " 'Colin 114',\n",
       " 'Collin 143',\n",
       " 'Colt 533',\n",
       " 'Colten 408',\n",
       " 'Colton 98',\n",
       " 'Conner 163',\n",
       " 'Connor 57',\n",
       " 'Conor 517',\n",
       " 'Conrad 758',\n",
       " 'Cooper 92',\n",
       " 'Cora 322',\n",
       " 'Corbin 262',\n",
       " 'Cordell 784',\n",
       " 'Corey 289',\n",
       " 'Corinne 841',\n",
       " 'Cornelius 988',\n",
       " 'Cortez 990',\n",
       " 'Cory 459',\n",
       " 'Courtney 269',\n",
       " 'Craig 588',\n",
       " 'Cristal 759',\n",
       " 'Cristian 150',\n",
       " 'Cristina 603',\n",
       " 'Cristofer 829',\n",
       " 'Cristopher 492',\n",
       " 'Cruz 367',\n",
       " 'Crystal 252',\n",
       " 'Cullen 781',\n",
       " 'Curtis 380',\n",
       " 'Cynthia 310',\n",
       " 'Cyrus 512',\n",
       " 'Dahlia 745',\n",
       " 'Daisy 161',\n",
       " 'Dakota 226',\n",
       " 'Dakota 267',\n",
       " 'Dale 839',\n",
       " 'Dalia 920',\n",
       " 'Dallas 334',\n",
       " 'Dalton 218',\n",
       " 'Damari 997',\n",
       " 'Damarion 725',\n",
       " 'Damaris 808',\n",
       " 'Damian 133',\n",
       " 'Damien 189',\n",
       " 'Damion 597',\n",
       " 'Damon 472',\n",
       " 'Dana 467',\n",
       " 'Dane 350',\n",
       " 'Dangelo 970',\n",
       " 'Dania 989',\n",
       " 'Danica 336',\n",
       " 'Daniel 5',\n",
       " 'Daniela 121',\n",
       " 'Daniella 303',\n",
       " 'Danielle 144',\n",
       " 'Danika 486',\n",
       " 'Danna 424',\n",
       " 'Danny 330',\n",
       " 'Dante 291',\n",
       " 'Daphne 536',\n",
       " 'Darian 602',\n",
       " 'Darien 843',\n",
       " 'Dario 850',\n",
       " 'Darion 890',\n",
       " 'Darius 318',\n",
       " 'Darnell 730',\n",
       " 'Darrell 634',\n",
       " 'Darren 385',\n",
       " 'Darryl 754',\n",
       " 'Darwin 788',\n",
       " 'Dashawn 810',\n",
       " 'Davian 690',\n",
       " 'David 14',\n",
       " 'Davin 594',\n",
       " 'Davion 440',\n",
       " 'Davis 410',\n",
       " 'Davon 712',\n",
       " 'Dawson 269',\n",
       " 'Dax 766',\n",
       " 'Dayami 750',\n",
       " 'Dayana 410',\n",
       " 'Dayanara 558',\n",
       " 'Dayton 484',\n",
       " 'Deacon 642',\n",
       " 'Dean 345',\n",
       " 'Deandre 460',\n",
       " 'Deangelo 739',\n",
       " 'Deanna 592',\n",
       " 'Deborah 809',\n",
       " 'Declan 347',\n",
       " 'Deegan 980',\n",
       " 'Deja 893',\n",
       " 'Delaney 209',\n",
       " 'Delilah 193',\n",
       " 'Demarcus 708',\n",
       " 'Demarion 905',\n",
       " 'Demetrius 520',\n",
       " 'Denise 364',\n",
       " 'Denisse 959',\n",
       " 'Dennis 370',\n",
       " 'Denzel 942',\n",
       " 'Deon 830',\n",
       " 'Derek 159',\n",
       " 'Dereon 882',\n",
       " 'Derick 733',\n",
       " 'Derrick 286',\n",
       " 'Deshawn 595',\n",
       " 'Desirae 983',\n",
       " 'Desiree 381',\n",
       " 'Desmond 413',\n",
       " 'Destinee 616',\n",
       " 'Destiney 714',\n",
       " 'Destiny 48',\n",
       " 'Devan 746',\n",
       " 'Deven 683',\n",
       " 'Devin 109',\n",
       " 'Devon 243',\n",
       " 'Devyn 847',\n",
       " 'Dexter 715',\n",
       " 'Diamond 449',\n",
       " 'Diana 137',\n",
       " 'Diego 68',\n",
       " 'Dillan 875',\n",
       " 'Dillon 264',\n",
       " 'Dixie 964',\n",
       " 'Diya 995',\n",
       " 'Dominic 93',\n",
       " 'Dominick 240',\n",
       " 'Dominik 577',\n",
       " 'Dominique 763',\n",
       " 'Dominique 785',\n",
       " 'Donald 331',\n",
       " 'Donavan 964',\n",
       " 'Donna 973',\n",
       " 'Donovan 214',\n",
       " 'Donte 782',\n",
       " 'Dorian 442',\n",
       " 'Douglas 424',\n",
       " 'Drake 230',\n",
       " 'Draven 670',\n",
       " 'Drew 244',\n",
       " 'Dulce 302',\n",
       " 'Duncan 717',\n",
       " 'Dustin 303',\n",
       " 'Dwayne 687',\n",
       " 'Dylan 31',\n",
       " 'Dylan 532',\n",
       " 'Ean 869',\n",
       " 'Easton 302',\n",
       " 'Eddie 438',\n",
       " 'Eden 232',\n",
       " 'Eden 896',\n",
       " 'Edgar 193',\n",
       " 'Edith 806',\n",
       " 'Eduardo 141',\n",
       " 'Edward 148',\n",
       " 'Edwin 175',\n",
       " 'Efrain 753',\n",
       " 'Eileen 760',\n",
       " 'Elaina 462',\n",
       " 'Elaine 742',\n",
       " 'Eleanor 256',\n",
       " 'Elena 211',\n",
       " 'Eli 100',\n",
       " 'Elian 817',\n",
       " 'Eliana 203',\n",
       " 'Elianna 1000',\n",
       " 'Elias 157',\n",
       " 'Eliezer 986',\n",
       " 'Elijah 22',\n",
       " 'Elisa 622',\n",
       " 'Elisabeth 525',\n",
       " 'Elise 218',\n",
       " 'Elisha 636',\n",
       " 'Eliza 328',\n",
       " 'Elizabeth 9',\n",
       " 'Ella 19',\n",
       " 'Elle 494',\n",
       " 'Ellen 672',\n",
       " 'Elliana 530',\n",
       " 'Ellie 167',\n",
       " 'Elliot 332',\n",
       " 'Elliott 386',\n",
       " 'Ellis 798',\n",
       " 'Elsa 724',\n",
       " 'Elsie 740',\n",
       " 'Elvis 713',\n",
       " 'Elyse 695',\n",
       " 'Emanuel 260',\n",
       " 'Emelia 960',\n",
       " 'Emely 305',\n",
       " 'Emerson 290',\n",
       " 'Emerson 474',\n",
       " 'Emery 468',\n",
       " 'Emery 744',\n",
       " 'Emilee 459',\n",
       " 'Emilia 401',\n",
       " 'Emiliano 315',\n",
       " 'Emilie 690',\n",
       " 'Emilio 281',\n",
       " 'Emily 3',\n",
       " 'Emma 1',\n",
       " 'Emmalee 743',\n",
       " 'Emmanuel 146',\n",
       " 'Emmett 547',\n",
       " 'Emmy 957',\n",
       " 'Enrique 279',\n",
       " 'Enzo 606',\n",
       " 'Eric 86',\n",
       " 'Erica 293',\n",
       " 'Erick 179',\n",
       " 'Erik 213',\n",
       " 'Erika 331',\n",
       " 'Erin 164',\n",
       " 'Ernest 789',\n",
       " 'Ernesto 412',\n",
       " 'Esmeralda 254',\n",
       " 'Esperanza 967',\n",
       " 'Essence 979',\n",
       " 'Esteban 366',\n",
       " 'Esther 274',\n",
       " 'Estrella 403',\n",
       " 'Ethan 3',\n",
       " 'Ethen 909',\n",
       " 'Eugene 691',\n",
       " 'Eva 114',\n",
       " 'Evan 38',\n",
       " 'Evangeline 450',\n",
       " 'Eve 655',\n",
       " 'Evelin 811',\n",
       " 'Evelyn 54',\n",
       " 'Everett 383',\n",
       " 'Evie 853',\n",
       " 'Ezekiel 241',\n",
       " 'Ezequiel 519',\n",
       " 'Ezra 292',\n",
       " 'Fabian 288',\n",
       " 'Faith 91',\n",
       " 'Fatima 249',\n",
       " 'Felicity 698',\n",
       " 'Felipe 535',\n",
       " 'Felix 354',\n",
       " 'Fernanda 423',\n",
       " 'Fernando 164',\n",
       " 'Finley 665',\n",
       " 'Finley 818',\n",
       " 'Finn 368',\n",
       " 'Finnegan 647',\n",
       " 'Fiona 347',\n",
       " 'Fisher 885',\n",
       " 'Fletcher 994',\n",
       " 'Frances 796',\n",
       " 'Francesca 491',\n",
       " 'Francis 656',\n",
       " 'Francisco 172',\n",
       " 'Franco 826',\n",
       " 'Frank 278',\n",
       " 'Frankie 762',\n",
       " 'Franklin 467',\n",
       " 'Freddy 707',\n",
       " 'Frederick 523',\n",
       " 'Frida 894',\n",
       " 'Gabriel 24',\n",
       " 'Gabriela 131',\n",
       " 'Gabriella 35',\n",
       " 'Gabrielle 79',\n",
       " 'Gael 296',\n",
       " 'Gage 144',\n",
       " 'Gaige 791',\n",
       " 'Garrett 160',\n",
       " 'Gary 426',\n",
       " 'Gauge 736',\n",
       " 'Gaven 918',\n",
       " 'Gavin 30',\n",
       " 'Gavyn 626',\n",
       " 'Gemma 888',\n",
       " 'Genesis 95',\n",
       " 'Genevieve 325',\n",
       " 'George 153',\n",
       " 'Georgia 312',\n",
       " 'Geovanni 934',\n",
       " 'Gerald 603',\n",
       " 'Gerardo 290',\n",
       " 'German 859',\n",
       " 'Gia 722',\n",
       " 'Giada 826',\n",
       " 'Giana 633',\n",
       " 'Giancarlo 635',\n",
       " 'Gianna 86',\n",
       " 'Gianni 543',\n",
       " 'Gideon 534',\n",
       " 'Gilbert 728',\n",
       " 'Gilberto 663',\n",
       " 'Gillian 832',\n",
       " 'Gina 854',\n",
       " 'Giovani 613',\n",
       " 'Giovanna 780',\n",
       " 'Giovanni 140',\n",
       " 'Giovanny 676',\n",
       " 'Giselle 136',\n",
       " 'Gisselle 653',\n",
       " 'Giuliana 875',\n",
       " 'Glenn 864',\n",
       " 'Gloria 431',\n",
       " 'Gordon 946',\n",
       " 'Grace 21',\n",
       " 'Gracelyn 683',\n",
       " 'Gracie 99',\n",
       " 'Grady 283',\n",
       " 'Graham 323',\n",
       " 'Grant 142',\n",
       " 'Grayson 188',\n",
       " 'Gregory 236',\n",
       " 'Greta 694',\n",
       " 'Gretchen 945',\n",
       " 'Greyson 381',\n",
       " 'Griffin 242',\n",
       " 'Guadalupe 296',\n",
       " 'Guillermo 470',\n",
       " 'Gunnar 540',\n",
       " 'Gunner 471',\n",
       " 'Gustavo 342',\n",
       " 'Gwendolyn 586',\n",
       " 'Hadassah 988',\n",
       " 'Hadley 414',\n",
       " 'Haiden 773',\n",
       " 'Hailee 456',\n",
       " 'Hailey 25',\n",
       " 'Hailie 990',\n",
       " 'Haleigh 728',\n",
       " 'Haley 101',\n",
       " 'Halle 506',\n",
       " 'Hallie 510',\n",
       " 'Hamza 760',\n",
       " 'Hana 827',\n",
       " 'Hanna 367',\n",
       " 'Hannah 17',\n",
       " 'Harley 415',\n",
       " 'Harley 592',\n",
       " 'Harmony 314',\n",
       " 'Harold 737',\n",
       " 'Harper 297',\n",
       " 'Harper 860',\n",
       " 'Harrison 219',\n",
       " 'Harry 644',\n",
       " 'Hassan 792',\n",
       " 'Haven 631',\n",
       " 'Hayden 126',\n",
       " 'Hayden 76',\n",
       " 'Haylee 234',\n",
       " 'Hayley 382',\n",
       " 'Haylie 497',\n",
       " 'Hazel 343',\n",
       " 'Heath 716',\n",
       " 'Heather 487',\n",
       " 'Heaven 275',\n",
       " 'Hector 181',\n",
       " 'Heidi 289',\n",
       " 'Heidy 946',\n",
       " 'Helen 371',\n",
       " 'Helena 612',\n",
       " 'Henry 78',\n",
       " 'Hezekiah 929',\n",
       " 'Hillary 715',\n",
       " 'Holden 358',\n",
       " 'Holly 365',\n",
       " 'Hope 225',\n",
       " 'Houston 880',\n",
       " 'Howard 903',\n",
       " 'Hudson 176',\n",
       " 'Hugh 998',\n",
       " 'Hugo 389',\n",
       " 'Humberto 805',\n",
       " 'Hunter 54',\n",
       " 'Ian 80',\n",
       " 'Ibrahim 589',\n",
       " 'Ignacio 814',\n",
       " 'Iliana 703',\n",
       " 'Imani 384',\n",
       " 'Immanuel 904',\n",
       " 'India 815',\n",
       " 'Ingrid 545',\n",
       " 'Ireland 834',\n",
       " 'Irene 636',\n",
       " 'Iris 341',\n",
       " 'Irvin 720',\n",
       " 'Isaac 37',\n",
       " 'Isabel 96',\n",
       " 'Isabela 533',\n",
       " 'Isabell 790',\n",
       " 'Isabella 2',\n",
       " 'Isabelle 93',\n",
       " 'Isai 734',\n",
       " 'Isaiah 42',\n",
       " 'Isaias 496',\n",
       " 'Ishaan 851',\n",
       " 'Isiah 446',\n",
       " 'Isis 607',\n",
       " 'Isla 623',\n",
       " 'Ismael 349',\n",
       " 'Israel 210',\n",
       " 'Issac 377',\n",
       " 'Itzel 383',\n",
       " 'Ivan 132',\n",
       " 'Ivy 298',\n",
       " 'Iyana 997',\n",
       " 'Izabella 231',\n",
       " 'Izabelle 958',\n",
       " 'Izaiah 428',\n",
       " 'Izayah 995',\n",
       " 'Jabari 771',\n",
       " 'Jace 180',\n",
       " 'Jacey 802',\n",
       " 'Jack 39',\n",
       " 'Jackson 32',\n",
       " 'Jacob 1',\n",
       " 'Jacoby 423',\n",
       " 'Jacqueline 152',\n",
       " 'Jacquelyn 794',\n",
       " 'Jada 110',\n",
       " 'Jade 129',\n",
       " 'Jaden 472',\n",
       " 'Jaden 88',\n",
       " 'Jadiel 874',\n",
       " 'Jadon 473',\n",
       " 'Jadyn 396',\n",
       " 'Jadyn 823',\n",
       " 'Jaeden 610',\n",
       " 'Jaelyn 330',\n",
       " 'Jaelynn 517',\n",
       " 'Jagger 954',\n",
       " 'Jaida 518',\n",
       " 'Jaiden 168',\n",
       " 'Jaiden 520',\n",
       " 'Jaidyn 565',\n",
       " 'Jaidyn 802',\n",
       " 'Jaime 321',\n",
       " 'Jair 852',\n",
       " 'Jairo 556',\n",
       " 'Jakayla 675',\n",
       " 'Jake 112',\n",
       " 'Jakob 309',\n",
       " 'Jakobe 930',\n",
       " 'Jalen 317',\n",
       " 'Jaliyah 544',\n",
       " 'Jamal 568',\n",
       " 'Jamar 718',\n",
       " 'Jamarcus 793',\n",
       " 'Jamari 391',\n",
       " 'Jamarion 485',\n",
       " 'James 17',\n",
       " 'Jameson 382',\n",
       " 'Jamie 263',\n",
       " 'Jamie 671',\n",
       " 'Jamir 767',\n",
       " 'Jamison 526',\n",
       " 'Jamiya 991',\n",
       " 'Jamya 746',\n",
       " 'Jan 961',\n",
       " 'Janae 795',\n",
       " 'Jane 390',\n",
       " 'Janelle 372',\n",
       " 'Janessa 571',\n",
       " 'Janet 716',\n",
       " 'Janiah 798',\n",
       " 'Janiya 359',\n",
       " 'Janiyah 356',\n",
       " 'Jaquan 806',\n",
       " 'Jaqueline 614',\n",
       " 'Jared 197',\n",
       " 'Jaron 819',\n",
       " 'Jarrett 894',\n",
       " 'Jase 643',\n",
       " 'Jasiah 752',\n",
       " 'Jaslene 376',\n",
       " 'Jaslyn 887',\n",
       " 'Jasmin 224',\n",
       " 'Jasmine 43',\n",
       " 'Jason 60',\n",
       " 'Jasper 452',\n",
       " 'Javier 166',\n",
       " 'Javion 573',\n",
       " 'Javon 447',\n",
       " 'Jax 692',\n",
       " 'Jaxon 154',\n",
       " 'Jaxson 268',\n",
       " 'Jay 395',\n",
       " 'Jayce 384',\n",
       " 'Jaycee 652',\n",
       " 'Jayda 258',\n",
       " 'Jaydan 886',\n",
       " 'Jayden 11',\n",
       " 'Jayden 176',\n",
       " 'Jaydin 745',\n",
       " 'Jaydon 434',\n",
       " 'Jayla 117',\n",
       " 'Jaylah 804',\n",
       " 'Jaylan 981',\n",
       " 'Jaylee 692',\n",
       " 'Jayleen 608',\n",
       " 'Jaylen 184',\n",
       " 'Jaylen 857',\n",
       " 'Jaylene 644',\n",
       " 'Jaylin 521',\n",
       " 'Jaylin 590',\n",
       " 'Jaylon 598',\n",
       " 'Jaylyn 810',\n",
       " 'Jaylynn 514',\n",
       " 'Jayson 341',\n",
       " 'Jayvion 977',\n",
       " 'Jayvon 950',\n",
       " 'Jazlene 831',\n",
       " 'Jazlyn 378',\n",
       " 'Jazlynn 730',\n",
       " 'Jazmin 205',\n",
       " 'Jazmine 242',\n",
       " 'Jazmyn 741',\n",
       " 'Jean 914',\n",
       " 'Jefferson 599',\n",
       " 'Jeffery 510',\n",
       " 'Jeffrey 198',\n",
       " 'Jenna 107',\n",
       " 'Jennifer 84',\n",
       " 'Jenny 601',\n",
       " 'Jensen 999',\n",
       " 'Jeramiah 876',\n",
       " 'Jeremiah 69',\n",
       " 'Jeremy 129',\n",
       " 'Jerimiah 985',\n",
       " 'Jermaine 501',\n",
       " 'Jerome 616',\n",
       " 'Jerry 320',\n",
       " 'Jesse 110',\n",
       " 'Jessica 59',\n",
       " 'Jessie 541',\n",
       " 'Jessie 626',\n",
       " 'Jesus 79',\n",
       " 'Jett 529',\n",
       " 'Jewel 896',\n",
       " 'Jillian 174',\n",
       " 'Jimena 398',\n",
       " 'Jimmy 355',\n",
       " 'Joanna 283',\n",
       " 'Joaquin 275',\n",
       " 'Jocelyn 67',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_names(filename):\n",
    "  \"\"\"\n",
    "  Given a file name for baby.html, returns a list starting with the year string\n",
    "  followed by the name-rank strings in alphabetical order.\n",
    "  ['2006', 'Aaliyah 91', Aaron 57', 'Abagail 895', ' ...]\n",
    "  \"\"\"\n",
    "  # open up the file provided\n",
    "  with open(filename) as f:\n",
    "    content = f.read()\n",
    "\n",
    "    # find the year\n",
    "    year = re.search(r'(?:<h1>Popular Names by Birth Year<\\/h1>.*)(\\d{4})',content).group(1)\n",
    "    # find the names and ranks\n",
    "    matches = re.findall(r'(?:<tr align=\"right\"><td>)(\\d+)(?:</td><td>)(\\w+)(?:</td><td>)(\\w+)', content)\n",
    "\n",
    "  # initialize the list to be returned\n",
    "  name_list = [year]\n",
    "  \n",
    "  # loop over the table of names\n",
    "  for match in matches:\n",
    "    # unpack the rank, male name, female name\n",
    "    rank, male, female = match\n",
    "    name_list.append(male + \" \" + rank)\n",
    "    name_list.append(female + \" \" + rank)\n",
    "\n",
    "  name_list = sorted(name_list)\n",
    "  return name_list\n",
    "\n",
    "\n",
    "filename = 'babynames/baby2008.html'\n",
    "extract_names(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkFy8w8LJW5i"
   },
   "source": [
    "## 2- NLTK\n",
    "\n",
    "NLTK is one of the major NLP packages in Python. It is targeted at learners rather than being a production library which makes a good starting point for our purposes.\n",
    "\n",
    "The first step is to import nltk then make sure that all the necessary files are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ot1hvTcxVAQL",
    "outputId": "3d1979bb-cce5-4b60-f87d-f03dc4feeb79"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUYVs5ICVAt1"
   },
   "source": [
    "If you are running on your personal machine (not within Google colab), you only need to do this once. \n",
    "\n",
    "NLTK offers a special module called \"book\" which can be imported using ```from nltk.book import *``` \n",
    "\n",
    "After printing a welcome message, it loads the text of several books (this will take a few seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLQtW_CLVmCg"
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tquXN7iJV16H"
   },
   "source": [
    "This module contains a number of Corpora that are ready to practice on. The Corpora are named: *text1* - *text9*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKG-wIKlV-Aq"
   },
   "outputs": [],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE6XRdkKWHcf"
   },
   "outputs": [],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr0CMsyZV-Np"
   },
   "source": [
    "###  Searching Text\n",
    "There are many ways to examine the context of a text apart from simply reading it. A concordance view shows us every occurrence of a given word, together with some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaDZGlTvWXWG",
    "outputId": "ede49f46-9e0b-4d82-ca2a-c92d59dfe278"
   },
   "outputs": [],
   "source": [
    "text1.concordance(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJblppzExJPn",
    "outputId": "3932f818-d534-469a-8018-029a351e37c2"
   },
   "outputs": [],
   "source": [
    "help(nltk.Text.concordance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eYpFKg32wv1"
   },
   "source": [
    "Try it yourself. Let's find more about the words: [\"often\", \"test\", \"extreme\"] is text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdqpzSCM2vwy"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sttxNYIXWXiu"
   },
   "source": [
    "A concordance permits us to see words in context. For example, we saw that monstrous occurred in contexts such as the \\_\\_\\_ pictures and a \\_\\_\\_ size . \n",
    "\n",
    "What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2eZZBBiuW7CF",
    "outputId": "155115fd-77cd-4485-97ed-4ea12b2c17c0"
   },
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrIWgQZFXCK6",
    "outputId": "b6918cf0-96d1-4817-fa17-cc8239670137"
   },
   "outputs": [],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7LpxuTDW7I4"
   },
   "source": [
    "Observe that we get different results for different texts. Austen uses this word quite differently from Melville; for her, monstrous has positive connotations, and sometimes functions as an intensifier like the word very.\n",
    "\n",
    "The method ```common_contexts``` allows us to examine just the contexts that are shared by two or more words, such as monstrous and very. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ttTzdVpXQqh",
    "outputId": "d81b11c1-7bf8-4e72-f783-4f0010eae68d"
   },
   "outputs": [],
   "source": [
    "text1.common_contexts([\"monstrous\", \"true\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPZbNAHCXQBw"
   },
   "source": [
    "Your Turn: pick another pair of words and compare their usage in two different texts, using the ```similar()``` and ```common_contexts()``` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3KW8t5D425d"
   },
   "outputs": [],
   "source": [
    "text1.similar(\"benevolent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-WzGnsq05kG"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# find the common contexts in which the words: great and dangerous are used in text1\n",
    "\n",
    "\n",
    "# find the common contexts in which the words: benevolent and fish are used in text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVihEk6L060k"
   },
   "source": [
    "It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. \n",
    "\n",
    "However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot. Each stripe represents an instance of a word, and each row represents the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "RBMOFFsJXcOR",
    "outputId": "58318f92-8386-460e-a4f3-9bb4ca5052cc"
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIwOZLyi6NqR"
   },
   "source": [
    "Try more words (e.g., liberty, constitution), and different texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h42HlzRJ6RWg"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn0QmfsrXcV0"
   },
   "source": [
    "\n",
    "For these plots to work, it is assumed that ```numpy``` and ```matplotlib``` are installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4Ac1p0lXuyY"
   },
   "source": [
    "###   Counting Vocabulary\n",
    "The most obvious fact about texts that emerges from the preceding examples is that they differ in the vocabulary they use. \n",
    "\n",
    "Let's begin by finding out the length of a text from start to finish, in terms of the words and punctuation symbols that appear. \n",
    "\n",
    "Let's find the number of words in the book of Genensis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYg6xoflYZ4B",
    "outputId": "651a769b-45cf-4958-ec38-443da5a7740f"
   },
   "outputs": [],
   "source": [
    "len(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tse16z5EYZI8"
   },
   "source": [
    "So Genesis has 44,764 words and punctuation symbols, or \"tokens.\" \n",
    "\n",
    "This includes duplicate tokens. To find the number of unique words, we use the ```set``` data structure as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZUY_xEwYqtz"
   },
   "outputs": [],
   "source": [
    "print(set(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhHUEUJG7tCn"
   },
   "source": [
    "and to get a sorted set, we use the Python ```sorted()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAFlnxij71qE",
    "outputId": "70c876fe-4bb0-432c-88c7-16b81d63369a"
   },
   "outputs": [],
   "source": [
    "print(sorted(set(text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3xn-ufTZB90",
    "outputId": "98c98836-48e4-4aaf-aa6a-9d96df18b058"
   },
   "outputs": [],
   "source": [
    "len(set(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWzz1HgGYq1u"
   },
   "source": [
    "Although it has 44,764 tokens, this book has only 2,789 distinct words, or \"word types.\" \n",
    "\n",
    "Now, let's calculate a measure of the lexical richness of the text. The next example shows us that the number of distinct words is just 6% of the total number of words, or equivalently that each word is used 16 times on average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7qEndcrZSWk",
    "outputId": "3f10039a-5e37-4147-b458-ff8794bb3c44"
   },
   "outputs": [],
   "source": [
    "len(set(text3)) / len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Q2UWXUw0SUT",
    "outputId": "84fabbef-9e21-4cf1-ac85-e4418c77e8ba"
   },
   "outputs": [],
   "source": [
    "len(text3)/len(set(text3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFIhQ-oaZSd6"
   },
   "source": [
    "Next, let's focus on particular words. We can count how often a word occurs in a text, and compute what percentage of the text is taken up by a specific word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuDVfwpWZhMI"
   },
   "outputs": [],
   "source": [
    "text3.count(\"smote\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfiqeKNPZoBc"
   },
   "outputs": [],
   "source": [
    "100 * text4.count('a') / len(text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRNw7RetZghy"
   },
   "source": [
    "Your Turn: How many times does the word *lol* appear in text5? How much is this as a percentage of the total number of words in this text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxhyP6uVZv0u"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdTUB9vdaZFh"
   },
   "source": [
    "To simplify things, let's create a functions for calculating lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8B9ybSUbaLwF"
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "  return len(set(text)) / len(text)\n",
    "\n",
    "def percentage(count, total):\n",
    "  return 100 * count / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSmjI5wfa023"
   },
   "source": [
    "We can go ahead and use these functions, let's find the number of tokens, the number of types, the lexical diversity for for the for the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "9rI2rb9Xbl4n",
    "outputId": "7e69156b-cf91-4214-f9e4-b0e749a2212f"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "print(f\"The type of brown is: {type(brown)}\")\n",
    "\n",
    "len(brown.words()), len(set(brown.words())), lexical_diversity(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SOYWtB2isvP"
   },
   "outputs": [],
   "source": [
    "brown.paras()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUlv45R6eHDj",
    "outputId": "9f8f4f4f-b2f6-454c-b837-e3565814aebb"
   },
   "outputs": [],
   "source": [
    "print(brown.raw()[:10])\n",
    "print(brown.words()[:10])\n",
    "print(brown.sents()[:2])\n",
    "print(brown.paras()[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9l8nU0eZwH2"
   },
   "source": [
    "\n",
    "\n",
    "###  Computing with Language: Simple Statistics\n",
    "Now, we pick up the question of what makes a text distinct, and use automatic methods to find characteristic words and expressions of a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkzW0Wh9fNRM"
   },
   "source": [
    "#### Frequency Distributions\n",
    "How can we automatically identify the words of a text that are most informative about the topic and genre of the text? \n",
    "\n",
    "Imagine how you might go about finding the 50 most frequent words of a book.\n",
    "\n",
    "Since we often need frequency distributions in language processing, NLTK provides built-in support for them. Let's use a ```FreqDist()``` to find the 50 most frequent words of Moby Dick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9FSqN-HhNZL"
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text1) \n",
    "print(fdist1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqNK27v-hQU0"
   },
   "outputs": [],
   "source": [
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUqv8nr9-Zrf"
   },
   "source": [
    "To find the frequency of a particular word, you can can access it using the word string as index, the output of FreqDist is a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q9XNu0LhVVw"
   },
   "outputs": [],
   "source": [
    "fdist1['whale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZXyqHN0hwX1"
   },
   "source": [
    "Your Turn: What are the top 20 tokens in text2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEXFhYl_iAxH"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhhLY9rp-vYH"
   },
   "source": [
    "Remember, we have calculated the frequencies in Lab 1 in pure Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roHngrruh-q1"
   },
   "source": [
    "Let's return to our task of finding words that characterize a text. \n",
    "\n",
    "Notice that the long words in *text4* reflect its national focus  constitutionally, transcontinental  whereas those in text5 reflect its informal content: boooooooooooglyyyyyy and yuuuuuuuuuuuummmmmmmmmmmm. \n",
    "\n",
    "Have we succeeded in automatically extracting words that typify a text? Well, these very long words are often hapaxes (i.e., unique) and perhaps it would be better to find frequently occurring long words. \n",
    "\n",
    "This seems promising since it eliminates frequent short words (e.g., the) and infrequent long words (e.g. antiphilosophists). \n",
    "\n",
    "Here are all words from the chat corpus that are longer than seven characters, that occur more than seven times:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZHgV0oojPR6"
   },
   "outputs": [],
   "source": [
    "fdist5 = FreqDist(text5)\n",
    "sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb7zYrIqjQB4"
   },
   "source": [
    "###   Counting Other Things\n",
    "Counting words is useful, but we can count other things too. For example, we can look at the distribution of word lengths in a text, by creating a ```FreqDist``` out of a long list of numbers, where each number is the length of the corresponding word in the text:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWv7VRzNjbgJ"
   },
   "outputs": [],
   "source": [
    "[len(w) for w in text1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRkt4uO5jemD",
    "outputId": "bba751c5-a28f-4028-8642-d0537d8f4e18"
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(len(w) for w in text1)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMDqZkk45l3H",
    "outputId": "624afe49-8acd-4ab2-f273-0937d1f449ba"
   },
   "outputs": [],
   "source": [
    "[w for w in text1 if len(w) == 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeXzRBNGjbmh"
   },
   "source": [
    "The result is a distribution containing a quarter of a million items, each of which is a number corresponding to a word token in the text. But there are at most only 20 distinct items being counted, the numbers 1 through 20, because there are only 20 different word lengths. I.e., there are words consisting of just one character, two characters, ..., twenty characters, but none with twenty one or more characters. \n",
    "\n",
    "\n",
    "One might wonder how frequent the different lengths of word are (e.g., how many words of length four appear in the text, are there more words of length five than length four, etc). We can do this as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdGQEJ93jt9O",
    "outputId": "b7e2387f-d73b-4c47-c232-4a16a6df5ed7"
   },
   "outputs": [],
   "source": [
    "fdist.most_common(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bh7iS5pzjuEx"
   },
   "outputs": [],
   "source": [
    "fdist.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZMPLEcUj3D5"
   },
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbx6aM4IjuNY"
   },
   "outputs": [],
   "source": [
    "fdist.freq(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53UO61kejuqu"
   },
   "source": [
    "From this we see that the most frequent word length is 3, and that words of length 3 account for roughly 50,000 (or 20%) of the words making up the book. \n",
    "\n",
    "\n",
    "Although we will not pursue it here, further analysis of word length might help us understand differences between authors, genres, or languages.\n",
    "\n",
    "### Functions Defined for NLTK's Frequency Distributions\n",
    "\n",
    "Example |\tDescription\n",
    "--- | ---\n",
    "`fdist = FreqDist(samples)` |\tcreate a frequency distribution containing the given samples\n",
    "`fdist[sample] += 1`\t| increment the count for this sample\n",
    "`fdist['monstrous']`\t| count of the number of times a given sample occurred\n",
    "`fdist.freq('monstrous')`\t| frequency of a given sample\n",
    "`fdist.N()`\t| total number of samples\n",
    "`fdist.most_common(n)`\t| the n most common samples and their frequencies\n",
    "`for sample in fdist:`\t| iterate over the samples\n",
    "`fdist.max()`\t| sample with the greatest count\n",
    "`fdist.tabulate()`\t| tabulate the frequency distribution\n",
    "`fdist.plot()`\t| graphical plot of the frequency distribution\n",
    "`fdist.plot(cumulative=True)`\t| cumulative plot of the frequency distribution\n",
    "`fdist1 \\|= fdist2`\t| update fdist1 with counts from fdist2\n",
    "`fdist1 < fdist2`\t| test if samples in fdist1 occur less frequently than in fdist2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EP4w0KU0koAb"
   },
   "source": [
    "###   Exercise 2.1\n",
    "\n",
    "a. How many words are there in text2? How many distinct words are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bv38eBjJkoMY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5536\\603206999.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There are {} words in text2\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"There are {} distinct words in text2\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text2' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"There are {} words in text2\".format(len(text2)))\n",
    "print(\"There are {} distinct words in text2\".format(len(set(text2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2KPIzh6k4Pa"
   },
   "source": [
    "b. Compare the lexical diversity scores for humor (text6) and romance fiction (text2). Which genre is more lexically diverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ7cPaR4lBWf"
   },
   "outputs": [],
   "source": [
    "print(\"The lexical diversity of text6 (humour) is {}\".format(lexical_diversity(text6)))\n",
    "print(\"The lexical diversity of text2 (romance) is {}\".format(lexical_diversity(text2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2x9VjyflEOk"
   },
   "source": [
    "c. Produce a dispersion plot of the four main protagonists in Sense and Sensibility: Elinor, Marianne, Edward, and Willoughby. What can you observe about the different roles played by the males and females in this novel? Can you identify the couples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "vTBH5MbVlN_f",
    "outputId": "731155b3-d302-4bb9-f13d-4b156f386a4e"
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "text2.dispersion_plot([\"Elinor\", \"Edward\", \"Marianne\", \"Willoughby\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngeLZ3blPIv"
   },
   "source": [
    "d. The first sentence of text3 is provided to you in the variable sent3. The index of *the* in sent3 is 1, because sent3[1] gives us 'the'. What are the indexes of the two other occurrences of this word in sent3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9NdXtTJhvLZ"
   },
   "outputs": [],
   "source": [
    "print(sent3)\n",
    "print([idx for idx, word in enumerate(sent3) if word == 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wscx1RDhlubI"
   },
   "source": [
    "e. Find all the four-letter words in the Chat Corpus (text5). With the help of a frequency distribution (FreqDist), show these words in decreasing order of frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ht2hQt31MoT",
    "outputId": "45e35430-252f-40ef-9df5-d7110abea2d2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "fd = FreqDist(w.lower() for w in text5 \n",
    "              if len(w)==4 and re.match(r\"[a-zA-Z]{4}\", w))\n",
    "fd.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grtzZehc1NId"
   },
   "source": [
    "f. Print all the uppercase words in Monty Python and the Holy Grail (text6)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwNmCCfD1hP_"
   },
   "outputs": [],
   "source": [
    "[word for word in text6.tokens if word.isupper()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re8IH7NG1hpT"
   },
   "source": [
    "g. Write expressions for finding all words in text6 that meet the conditions listed below. The result should be in the form of a list of words\n",
    "\n",
    "* Ending in ise\n",
    "* Containing the letter z\n",
    "* Containing the sequence of letters pt\n",
    "* Having all lowercase letters except for an initial capital (i.e., titlecase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qs0LYav51yWI"
   },
   "outputs": [],
   "source": [
    "# Ending in ise\n",
    "print([word for word in text6.tokens if word[-3:] == 'ise'])\n",
    "\n",
    "# Containing the letter z\n",
    "print([word for word in text6.tokens if 'z' in word])\n",
    "\n",
    "# Containing the sequence of letters pt\n",
    "print([word for word in text6.tokens if 'pt' in word])\n",
    "\n",
    "# Having all lowercase letters except for an initial capital (i.e., titlecase)\n",
    "print([word for word in text6.tokens if word.istitle()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osQr8oNSKqlz"
   },
   "source": [
    "### Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpzAfV6C1yve"
   },
   "source": [
    "Practical work in Natural Language Processing typically uses large bodies of linguistic data, or corpora.\n",
    "\n",
    "### Accessing Text Corpora\n",
    "A text corpus is a large body of text. Many corpora are designed to contain a careful balance of material in one or more genres. \n",
    "\n",
    "We look at various pre-defined texts that we accessed by typing ```from nltk.book import *```. However, since we want to be able to work with other texts, we now examine a variety of text corpora. \n",
    "\n",
    "NLTK provides access to a number of well known Corpora as well as allows us to build our own. We will examine the Corpora that are made availabe to us first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YI1yhqsEEDz"
   },
   "source": [
    "**Gutenberg Corpus**\n",
    "\n",
    "NLTK includes a small selection of texts from the Project Gutenberg electronic text archive, which contains some 25,000 free electronic books, hosted at http://www.gutenberg.org/. which can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg5UIaYwEUE0"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n1aChZbEkSz"
   },
   "source": [
    "we can find the list of ducuments included in this corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erLZKZYFEsJR",
    "outputId": "89e2f632-f5bc-485c-9f5e-5cb07885d2ef"
   },
   "outputs": [],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMEAgM_tHwPo",
    "outputId": "96fbecec-f4a9-4cee-eed6-71671e24914b"
   },
   "outputs": [],
   "source": [
    "print(type(gutenberg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xn9QnAtCF83O"
   },
   "source": [
    "The Corpus is an instance of the class ```PlaintextCorpusReader```\n",
    "\n",
    "To be able to use the ```concordance``` method we saw earlier, we need to load the words as a ```Text``` object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLk96d4WF7jA"
   },
   "outputs": [],
   "source": [
    "emma = gutenberg.words(gutenberg.fileids()[0])\n",
    "emma_text = nltk.Text(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OsrZKu6eFoGF",
    "outputId": "f5dd6fa4-17dc-4e8b-9ecf-a34d0931e262"
   },
   "outputs": [],
   "source": [
    "emma_text.concordance(\"dry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GR6jH-KgGw78"
   },
   "source": [
    "### Exercise 2.2\n",
    "\n",
    "Find the longest word in the file *chesterton-thursday.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2lrTMlXG_fD"
   },
   "outputs": [],
   "source": [
    "chesterton_thursday = gutenberg.words('chesterton-thursday.txt')\n",
    "longest = ''\n",
    "for word in chesterton_thursday:\n",
    "  if len(word) > len(longest):\n",
    "    longest = word\n",
    "longest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MWCxYnFHmlW"
   },
   "source": [
    "NLTK offers a number of of Corpora, worth mentioning are the following:\n",
    "\n",
    "package name | Corpus name \n",
    "--- | --- \n",
    "webtext | Web Text \n",
    "nps_chat | Instant messaging sessions \n",
    "brown | Brown Corpus \n",
    "reuters | Reuters Corpus \n",
    "inaugural | Inaugural Address Corpus \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h0iYe-QLXUG"
   },
   "source": [
    "Summary of basic Corpus methods defined in NLTK\n",
    "\n",
    "Example\t| Description\n",
    "--- | ---\n",
    "`fileids()`\t| the files of the corpus\n",
    "`fileids([categories])`\t| the files of the corpus corresponding to these categories \n",
    "`categories()`\t| the categories of the corpus\n",
    "`categories([fileids])`\t| the categories of the corpus corresponding to these files\n",
    "`raw()`\t| the raw content of the corpus\n",
    "`raw(fileids=[f1,f2,f3]) `|\tthe raw content of the specified files\n",
    "`raw(categories=[c1,c2])` |\tthe raw content of the specified categories\n",
    "`words()`\t| the words of the whole corpus\n",
    "`words(fileids=[f1,f2,f3])` |\tthe words of the specified fileids\n",
    "`words(categories=[c1,c2])`|\tthe words of the specified categories\n",
    "`sents()`\t| the sentences of the whole corpus\n",
    "`sents(fileids=[f1,f2,f3])` |\tthe sentences of the specified fileids\n",
    "`sents(categories=[c1,c2])`\t| the sentences of the specified categories\n",
    "`abspath(fileid)`\t| the location of the given file on disk\n",
    "`encoding(fileid)`\t| the encoding of the file (if known)\n",
    "`open(fileid)`\t| open a stream for reading the given corpus file\n",
    "`root` |\tif the path to the root of locally installed corpus\n",
    "`readme()`\t| the contents of the README file of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFBoDWYOJsU4"
   },
   "source": [
    "NLTK's built in Corpora are good for learning the concepts, but eventually, we would like to operate on our own data. \n",
    "\n",
    "Text data in the wild comes in many formats including: plain text files, PDF, DOCX, html, json (twitter), etc.\n",
    "\n",
    "\n",
    "The Python ecosystem is rich with packages that makes it easy to access the different types of files and APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMx4wapt3LWq"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "For analysis and further processing, we want to break up the string into words and punctuation. This step is called tokenization, and it produces our familiar structure, a list of words and punctuation.\n",
    "\n",
    "A sentence or data can be split into words using the method ```word_tokenize()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86aP9BY2NtRl",
    "outputId": "29f34c2e-c6e2-4b93-fd34-3897ddf4cd52"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e9TZtPnPd5t",
    "outputId": "97aae98e-613f-4cee-b67f-ccc8539cff2c"
   },
   "outputs": [],
   "source": [
    "word_text = nltk.Text(word_tokens)\n",
    "word_text.vocab().most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx19Q3YUQ8tR"
   },
   "source": [
    "The same principle can be applied to sentences. Simply change the to\n",
    "```sent_tokenize()``` We have added two sentences to the variable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKZkCb4LRNPD",
    "outputId": "c7266c61-5ee3-4a5f-b2ee-06b071c4d811"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feC19DXnNata"
   },
   "source": [
    "### Exercise 2.3\n",
    "\n",
    "What is the longest word in the provided article bellow? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rdejqn5L96A"
   },
   "outputs": [],
   "source": [
    "# We grab the article HTML\n",
    "from urllib import request\n",
    "\n",
    "url = \"https://www.technologyreview.com/2021/03/17/1020811/better-tech-government-pandemic-united-states/\"\n",
    "\n",
    "response = request.urlopen(url)\n",
    "print(response.code)\n",
    "\n",
    "raw = response.read().decode('utf8')\n",
    "print(type(raw))\n",
    "print(len(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEfp-j3RT_zZ"
   },
   "outputs": [],
   "source": [
    "# Then we parse the response\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "parsed_text = BeautifulSoup(raw, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HEafw3oUwBn"
   },
   "outputs": [],
   "source": [
    "def find_longest_word(tokens):\n",
    "  longest = ''\n",
    "  for token in tokens:\n",
    "    if len(token) > len(longest):\n",
    "      longest = token\n",
    "  return longest\n",
    "\n",
    "tokenized_text = word_tokenize(parsed_text[241122:253902])\n",
    "\n",
    "find_longest_word(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9X1M2J03TIr"
   },
   "source": [
    "### Stop words\n",
    "English text may contain stop words like the, is, are. Stop words can be filtered from the text to be processed. There is no universal list of stop words in NLP research, however the NLTK module contains a list of stop words. Now you will learn how to remove stop words using the NLTK. \n",
    "\n",
    "We start with the code from the previous section with tokenized words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIJ7tTTRNN0t"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jw4lYR7ESR6M",
    "outputId": "5bee0f5a-5860-4d98-e563-8c413bd1bcd0"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords # We imported auxiliary corpus provided with NLTK\n",
    "\n",
    "text = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "words = word_tokenize(data.lower())\n",
    "\n",
    "wordsFiltered = [w for w in words if w not in stopWords]\n",
    "\n",
    "(wordsFiltered) # Print the filtered text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AeM2rusPTSb3"
   },
   "source": [
    "### Exercise 2.4\n",
    "\n",
    "using the variable from `parsed_text` from Exercise 3, find the most common 10 words before and after removing the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRd03-nVTopA"
   },
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(parsed_text[241122:253902])\n",
    "tokenized_text = [word for word in tokenized_text if word.isalpha()]\n",
    "\n",
    "top10_words_before = FreqDist(tokenized_text).most_common(10)\n",
    "print([word for word, freq in top10_words_before])\n",
    "\n",
    "tokenized_text_filtered = [word for word in tokenized_text if word not in stop_words]\n",
    "top10_words_after = FreqDist(tokenized_text_filtered).most_common(10)\n",
    "print([word for word, freq in top10_words_after])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0DYHrw3JZ9C"
   },
   "source": [
    "### Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rdQ0NUbYTOa"
   },
   "source": [
    "Stemmers\n",
    "\n",
    "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer you should use one of these in preference to crafting your own using regular expressions, since these handle a wide range of irregular cases. The Porter and Lancaster stemmers follow their own rules for stripping affixes. \n",
    "\n",
    "Observe that the Porter stemmer correctly handles the word lying (mapping it to lie), while the Lancaster stemmer does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuG8dzPAUEEB"
   },
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    " is no basis for a system of government.  Supreme executive power derives from\n",
    " a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqzIlLjoUXbU"
   },
   "outputs": [],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "\n",
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dra2Jw0xUEz7"
   },
   "source": [
    "Stemming is not a well-defined process, and we typically pick the stemmer that best suits the application we have in mind. \n",
    "\n",
    "**NOTE:**\n",
    "Stemming is an important step for traditional statistical machine learning systems, however for deep learning based methods, stemming is not used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hiV2YEaKeMX"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N-qEAvMYXNN"
   },
   "source": [
    "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the above stemmers. Notice that it doesn't handle lying, but it converts women to woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SW-xDzkjWeyF"
   },
   "outputs": [],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D_VMczyWh1h"
   },
   "source": [
    "Compare the results of stemming and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bn75rgXW6d4"
   },
   "source": [
    "### Exercise 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UUNd-k5KhQm"
   },
   "source": [
    "Read the gutenberg Corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ6iFD2mgkdB"
   },
   "outputs": [],
   "source": [
    "# I picked bryant-stories.txt\n",
    "wh_words = ['why', 'who', 'which', 'what', 'where', 'when', 'how']\n",
    "tokens = gutenberg.words('bryant-stories.txt')\n",
    "wh_tokens = [token for token in tokens if token.lower() in wh_words]\n",
    "print(wh_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftoo3AyPLkEX"
   },
   "source": [
    "## 3- Pandas\n",
    "\n",
    "Pandas is a ubiquitous python library for efficiently handling tabular data. It is used loading and cleaning up text data.\n",
    "\n",
    "**Note**: \n",
    "* The practice here is minimal, for a more comprehensive coverage of Pandas for textual data check \n",
    "https://www.youtube.com/watch?v=hI1RZgbcW7g\n",
    "* Being able to utilize basic Pandas fuctionality will become useful in the assignments.\n",
    "\n",
    "\n",
    "\n",
    "Pandas offers two data types, Series and DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSC-MfUVBqE5",
    "outputId": "0d89842e-31fe-45c2-baf5-2d0f554e9095"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzvJPI3wB-wD",
    "outputId": "5624a5d2-1a53-4bea-c7f7-f5a31c7a43de"
   },
   "outputs": [],
   "source": [
    "# data is stored as object internally\n",
    "print(pd.Series([\"a\", \"b\", \"c\"]))\n",
    "\n",
    "# data is stored as string internally\n",
    "print(pd.Series([\"a\", \"b\", \"c\"], dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-z9tpxPCngJ",
    "outputId": "c778be2c-32cb-4541-82d9-3fa6fe564c98"
   },
   "outputs": [],
   "source": [
    "s = pd.Series([\"a\", None, \"b\"], dtype=\"string\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-jHgbp_vC8yQ",
    "outputId": "3a8774e7-5703-4fc6-c0ab-68de0a8df8be"
   },
   "outputs": [],
   "source": [
    "print(s.str.count(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "672mJDCAC_i7",
    "outputId": "9949b7f4-4182-420f-cce5-bac903f31ddb"
   },
   "outputs": [],
   "source": [
    "print(s.dropna().str.count(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGaXhcxTDPgj",
    "outputId": "17fc7601-afa9-4a3a-a39b-dbce61b11a8d"
   },
   "outputs": [],
   "source": [
    "s.str.isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vf9A442lDVJ8",
    "outputId": "48a1e961-a0c7-4095-e530-cb2f4a187b2b"
   },
   "outputs": [],
   "source": [
    "s.str.match(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKcaDhg3DbKE"
   },
   "source": [
    "### String operations\n",
    "\n",
    "Series is equipped with a set of string processing methods that make it easy to operate on each element of the array. e.g. methods to exclude missing/NA values automatically. \n",
    "\n",
    "These are accessed via the str attribute and generally have names matching the equivalent (scalar) built-in string methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCPB2FH3Dfgj",
    "outputId": "bcbe47d5-3e64-4e3c-c779-b92aa16e56dc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "s = pd.Series([\"A\", \"B\", \"C\", \"Aaba\", \"Baca\", np.nan, \"CABA\", \"dog\", \"cat\"], dtype=\"string\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jolWaFiTEEF9",
    "outputId": "9bae1f49-7951-4531-932e-a89ec8d2f18b"
   },
   "outputs": [],
   "source": [
    "print(s.str.lower())\n",
    "\n",
    "print(s.str.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVfGJkJTEXfO",
    "outputId": "598c7282-71e5-4b80-a94b-4927193693d8"
   },
   "outputs": [],
   "source": [
    "s.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1SO6ZUfEsgV"
   },
   "source": [
    "### Splitting and replacing strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajMdPjN8Ex_u",
    "outputId": "1b2dfef0-3e3d-4c4b-d1b5-5380124214a6"
   },
   "outputs": [],
   "source": [
    "s2 = pd.Series([\"a_b_c\", \"c_d_e\", np.nan, \"f_g_h\"], dtype=\"string\")\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrI5ebrrE0Ub",
    "outputId": "e9a7f72c-ead2-4b67-9c4f-a4deda076bf4"
   },
   "outputs": [],
   "source": [
    "s2.str.split(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EC0dOFOYE7vA"
   },
   "source": [
    "Elements in the split lists can be accessed using get or [] notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ap-TRcE9GQ",
    "outputId": "35a34569-5879-46f0-c1ae-005c5afa4300"
   },
   "outputs": [],
   "source": [
    "s2.str.split(\"_\").str.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3P1z-pQE7tG"
   },
   "source": [
    "It is easy to expand this to return a DataFrame using expand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RmiZ509QFNcE",
    "outputId": "9e98efe5-3e23-494c-f0ff-05935e02121c"
   },
   "outputs": [],
   "source": [
    "print(s2.str.split(\"_\", expand=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGHPIZ83Fav7"
   },
   "source": [
    "It is also possible to limit the number of splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K02KASnEFe1L",
    "outputId": "26527aee-7e52-42b3-a2fa-576e21fc354f"
   },
   "outputs": [],
   "source": [
    "print(s2.str.split(\"_\", expand=True, n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeufdO9c2zHC"
   },
   "source": [
    "## 4- Spacy - NLP in production\n",
    "\n",
    "SpaCy is another open-source library for Natural Language Processing (NLP) in Python.\n",
    "\n",
    "spaCy is designed specifically for production (not for learning the concepts) use and helps you build applications that process and understand large volumes of text.\n",
    "\n",
    "The way tokenization is handled in Spacy is summerized in the following diagram:\n",
    "\n",
    "![](https://spacy.io/tokenization-9b27c0f6fe98dcb26239eba4d3ba1f3d.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nncZ9J_ZkWG2"
   },
   "outputs": [],
   "source": [
    "# upgrade spacy to the latest version\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJfwYlYdh4vI",
    "outputId": "e9f89c4f-2928-4383-d2ac-9bbe93cfe934"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple isn't looking at buying U.K. startup for $1 billion\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HgQvF-SkC93"
   },
   "source": [
    "Lemmatization in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orx3qdrbkBvA",
    "outputId": "9c776751-dc87-4b8a-8178-fd4f933ea42b"
   },
   "outputs": [],
   "source": [
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nta_Rb2rmzBS"
   },
   "source": [
    "Notice the difference between the tokens and lemmas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNAlp_Nuymjc"
   },
   "source": [
    "## 5- WordClouds\n",
    "\n",
    "Word clouds are a visualization technique which represent the frequency or the importance of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZHqvYY-ysDW",
    "outputId": "2ee4e2a6-e634-4f85-b1ba-3b419178185a"
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "s6Qst4m5ywTh",
    "outputId": "5bda8649-6e14-4af0-b08b-0cdb99050e60"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "test_corpus2 = \" \".join(text1.tokens)\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\").generate(test_corpus2)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLhszjThlgDL"
   },
   "source": [
    "### Optional Excercise\n",
    "\n",
    "Generate a word cloud for the book at https://www.gutenberg.org/cache/epub/19017/pg19017.txt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTzkb5z6tVl9"
   },
   "source": [
    "## 6- References \n",
    "* https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XTCNgVBE3YQZ",
    "6E9bskKo5k5Y",
    "Fa-S1pnw50Qa",
    "zm5Qe4PB60Sa",
    "pv3ab3629Sd0",
    "sqhnW1As97Xe",
    "lORICw0X_tpK",
    "9HiooV16Dz7m",
    "GTzkb5z6tVl9",
    "Mh3mUKwYOZ3F",
    "L2YrJ_P9Q_2J",
    "fo4KyNiZSh0z",
    "wHfLkDU2TQav",
    "6XV0ZgDvUxSH",
    "nBSnxX3iaLf4",
    "9F8xg2kua2rg",
    "PHUvi546qvDW",
    "0bnGSit5q8jl",
    "oBuW6FNWcCre"
   ],
   "name": "Lab01-2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "90567bd864de7d7c905b0171921e3d51ee8e110edc8c6dde1df5a1ca6f9f81a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
